{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e578cf87-1acb-4f69-8e8f-29c53bda4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytz\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "import os\n",
    "import hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1d08cc4-f801-4b7c-bdae-59004c5fa75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD embeddings\n",
    "X_all=np.load(\"./data/embeddings_United States_01.npy\")\n",
    "\n",
    "# LOAD ran tSNE result\n",
    "X_2d_all=np.load(\"./data/tsne_United States_01.npy\")\n",
    "\n",
    "# LOAD submissions from US in Jan\n",
    "subset_country=pd.read_csv(\"./data/USsub_January.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3559aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# # ----------------------------\n",
    "# # 6. t-SNE reduction\n",
    "# # ----------------------------\n",
    "# print(\"Fitting t-SNE (this may take a while)...\")\n",
    "# tsne = TSNE(n_components=2, random_state=42, metric='cosine', n_jobs=-1, init='pca', verbose=2)\n",
    "# X_2d_all = tsne.fit_transform(X_all)\n",
    "# print(\"t-SNE complete.\")\n",
    "\n",
    "\n",
    "# Save t-SNE results\n",
    "# tsne_save_file = f\"tsne_{target_country}_{target_month}.npy\"\n",
    "# os.makedirs(\"tsne_results\", exist_ok=True)\n",
    "# np.save(os.path.join(\"tsne_results\", tsne_save_file), X_2d_all)\n",
    "# print(f\"t-SNE coordinates saved to tsne_results/{tsne_save_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3143a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters found: 2863\n"
     ]
    }
   ],
   "source": [
    "subset_country['embedding_2d'] = list(X_2d_all)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. HDBSCAN clustering\n",
    "# ----------------------------\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10, metric='euclidean')\n",
    "cluster_labels = clusterer.fit_predict(X_2d_all)\n",
    "subset_country[\"cluster\"] = cluster_labels\n",
    "print(f\"Number of clusters found: {len(np.unique(cluster_labels))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "697ca408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming subset_country already has columns:\n",
    "# 'local_hour', 'cluster', 'embedding_2d' (t-SNE 2D)\n",
    "hours = range(24)\n",
    "clusters = np.unique(subset_country['cluster'])\n",
    "clusters = clusters[clusters != -1]  # ignore noise\n",
    "\n",
    "# Track cumulative size of each cluster\n",
    "cumulative_sizes = {c: 0 for c in clusters}\n",
    "# Track hourly additions\n",
    "hourly_additions = {c: [] for c in clusters}\n",
    "\n",
    "for hour in hours:\n",
    "    df_hour = subset_country[subset_country['local_hour'] == hour]\n",
    "    for c in clusters:\n",
    "        n_points = np.sum(df_hour['cluster'] == c)\n",
    "        hourly_additions[c].append(n_points)\n",
    "        cumulative_sizes[c] += n_points\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "hourly_df = pd.DataFrame(hourly_additions, index=hours)\n",
    "hourly_df.index.name = 'hour'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aedcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Parameters\n",
    "# ============================================================\n",
    "TIME_POINTS = np.arange(0, 24, 4)   # 6 time points\n",
    "ALPHA_CLUSTER = 0.6\n",
    "ALPHA_LINE = 0.15\n",
    "\n",
    "# ============================================================\n",
    "# Load embeddings + clustering\n",
    "# ============================================================\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n",
    "subset_country[\"cluster\"] = clusterer.fit_predict(X_2d_all)\n",
    "\n",
    "clusters = np.unique(subset_country[\"cluster\"])\n",
    "clusters = clusters[clusters != -1]\n",
    "\n",
    "# Global axis limits for t-SNE\n",
    "x_min, x_max = X_2d_all[:, 0].min(), X_2d_all[:, 0].max()\n",
    "y_min, y_max = X_2d_all[:, 1].min(), X_2d_all[:, 1].max()\n",
    "\n",
    "# ============================================================\n",
    "# Hourly cluster counts\n",
    "# ============================================================\n",
    "hourly_additions = {c: [] for c in clusters}\n",
    "\n",
    "for h in range(24):\n",
    "    df_h = subset_country[subset_country[\"local_hour\"] == h]\n",
    "    for c in clusters:\n",
    "        hourly_additions[c].append(np.sum(df_h[\"cluster\"] == c))\n",
    "\n",
    "hourly_df = pd.DataFrame(hourly_additions, index=range(24))\n",
    "hourly_df_6 = hourly_df.loc[TIME_POINTS]\n",
    "\n",
    "total_posts_per_cluster = hourly_df.sum(axis=0)\n",
    "ranked_clusters = total_posts_per_cluster.sort_values(ascending=False).index\n",
    "\n",
    "# ============================================================\n",
    "# Figure layout\n",
    "# ============================================================\n",
    "plt.rcParams.update({\n",
    "    \"font.size\": 11,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False\n",
    "})\n",
    "\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "gs = GridSpec(1, 3, figure=fig, width_ratios=[1.6, 1, 1], wspace=0.2)\n",
    "\n",
    "# ============================================================\n",
    "# (A) t-SNE cluster snapshots (6 time points)\n",
    "# ============================================================\n",
    "gsA = gs[0, 0].subgridspec(2, 3, wspace=0.05, hspace=0.15)\n",
    "\n",
    "for i, hour in enumerate(TIME_POINTS):\n",
    "    ax = fig.add_subplot(gsA[i // 3, i % 3])\n",
    "    df_h = subset_country[subset_country[\"local_hour\"] == hour]\n",
    "\n",
    "    if len(df_h) > 0:\n",
    "        X_h = np.vstack(df_h[\"embedding_2d\"].values)\n",
    "        labels = df_h[\"cluster\"].values\n",
    "\n",
    "        mask = labels != -1\n",
    "        ax.scatter(\n",
    "            X_h[mask, 0], X_h[mask, 1],\n",
    "            c=labels[mask] % 20,\n",
    "            cmap=\"tab20\",\n",
    "            s=12,\n",
    "            alpha=ALPHA_CLUSTER\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"h = {hour}\", fontsize=10)\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# (B) Power-law scaling of cluster sizes\n",
    "# ============================================================\n",
    "axB = fig.add_subplot(gs[0, 1])\n",
    "\n",
    "bins = np.logspace(\n",
    "    np.log10(total_posts_per_cluster.min()),\n",
    "    np.log10(total_posts_per_cluster.max()),\n",
    "    40\n",
    ")\n",
    "hist, edges = np.histogram(total_posts_per_cluster, bins=bins)\n",
    "centers = (edges[:-1] + edges[1:]) / 2\n",
    "\n",
    "axB.loglog(centers, hist, \"o\")\n",
    "axB.set_xlabel(\"Cluster size (total posts)\")\n",
    "axB.set_ylabel(\"Number of clusters\")\n",
    "# axB.set_title(\"Cluster size distribution\")\n",
    "\n",
    "mask = hist > 0\n",
    "slope, _ = np.polyfit(\n",
    "    np.log10(centers[mask]),\n",
    "    np.log10(hist[mask]),\n",
    "    1\n",
    ")\n",
    "axB.text(\n",
    "    0.05, 0.05,\n",
    "    rf\"$P(s)\\sim s^{{-{abs(slope):.2f}}}$\",\n",
    "    transform=axB.transAxes\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# (C) Cumulative cluster growth (6 time points)\n",
    "# ============================================================\n",
    "axC = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "for c in ranked_clusters:\n",
    "    axC.plot(\n",
    "        np.arange(24),\n",
    "        np.cumsum(hourly_df[c]),\n",
    "        color=\"black\",\n",
    "        alpha=ALPHA_LINE\n",
    "    )\n",
    "\n",
    "axC.set_xlabel(\"Local hour\")\n",
    "axC.set_ylabel(\"Cumulative submissions\")\n",
    "# axC.set_title(\"Cumulative growth\")\n",
    "\n",
    "# ============================================================\n",
    "# Final layout\n",
    "# ============================================================\n",
    "# fig.suptitle(\n",
    "#     f\"Temporal organization and scaling of semantic clusters\\n\"\n",
    "#     f\"{target_country}, {target_month}\",\n",
    "#     y=1.05,\n",
    "#     fontsize=14\n",
    "# )\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.text(0.12, 0.95, \"(A)\", fontsize=20, weight=\"bold\")\n",
    "fig.text(0.44, 0.95, \"(B)\", fontsize=20, weight=\"bold\")\n",
    "fig.text(0.69, 0.95, \"(C)\", fontsize=20, weight=\"bold\")\n",
    "plt.savefig(\"Figure6.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssq_env_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
